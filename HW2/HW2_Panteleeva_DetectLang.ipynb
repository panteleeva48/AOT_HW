{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import codecs\n",
    "import collections\n",
    "import sys\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Скачивание статей из Википедии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang, n=10): # функция для скачивания статей из википедии\n",
    "    wikipedia.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wikipedia.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            print('Skipping page {}'.format(page_name))\n",
    "            continue\n",
    "\n",
    "        wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Языки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "langs = ('kk', 'uk', 'be', 'fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Казахский (kk)\n",
    "* Украинский (uk)\n",
    "* Белорусский (be)\n",
    "* Французский (fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание корпусов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kk 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Княжицька сільська рада\n",
      "Skipping page Задорожна\n",
      "Skipping page Фреден\n",
      "Skipping page Демкович\n",
      "Skipping page Сенан\n",
      "Skipping page Ламбет (значення)\n",
      "uk 94\n",
      "Skipping page Монькі (значэнні)\n",
      "Skipping page Уладычына\n",
      "Skipping page Асавец\n",
      "be 97\n",
      "Skipping page Actif\n",
      "Skipping page Mauvaise Fille (film, 1991)\n",
      "Skipping page Pleurodema elegans\n",
      "Skipping page Gold\n",
      "Skipping page Hernie\n",
      "Skipping page La Pucelle d'Orléans (Schiller)\n",
      "Skipping page Reaper\n",
      "fr 93\n"
     ]
    }
   ],
   "source": [
    "wiki_texts = {}\n",
    "for lang in langs:\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "    print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Первый метод: частотные слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Чистим текст от тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "    no_tags_text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    no_space_sequences_text = re.sub('  +', ' ', no_tags_text)\n",
    "    return no_space_sequences_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# удаляем знаки препинания\n",
    "def tokenize(text):\n",
    "    punct_extended = string.punctuation + '«»—…“”'\n",
    "    table = str.maketrans({ch: None for ch in punct_extended})\n",
    "    return [word.translate(table) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Список частотных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frequent_list(langs, wiki_texts):\n",
    "    lang_freqs = {}\n",
    "    for lang in wiki_texts:\n",
    "        corpus = wiki_texts[lang]\n",
    "        lang_freqs[lang] = collections.defaultdict(lambda: 0)\n",
    "        for article in corpus:\n",
    "            for word in tokenize(remove_tags(article.replace('\\n', '').lower())):\n",
    "                lang_freqs[lang][word] += 1\n",
    "    return lang_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Фильтрация повторяющихся токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_tokens(d):\n",
    "    result = {}\n",
    "    toks = []\n",
    "    for key, value in d.items():\n",
    "        result[key] = {}\n",
    "        toks = toks + list(value.keys())\n",
    "    duplicates  = [item for item, count in collections.Counter(toks).items() if count > 1]\n",
    "    for key2, value2 in d.items():\n",
    "        for key3, value3 in value2.items():\n",
    "            if key3 not in duplicates:\n",
    "                result[key2].update({key3 : value3})\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Список  самых частотных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_frequent(d, keys, num):\n",
    "    most_freqs = {}\n",
    "    for k in keys:\n",
    "        most_freqs[k] = set(sorted(d[k], key=lambda w: d[k][w], reverse=True)[:num])\n",
    "    return most_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_freq(langs, wiki_texts):\n",
    "    # составляем частотный список\n",
    "    lang_freqs = frequent_list(langs, wiki_texts)\n",
    "    # фильтруем токены\n",
    "    lang_freqs_filtered = filter_tokens(lang_freqs)\n",
    "    # достаём самые частотные\n",
    "    most_freq = most_frequent(lang_freqs_filtered, langs, 300)\n",
    "    return most_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Определение языка корпусным методом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_method(text, most_freq):\n",
    "    result = {}\n",
    "    for lang in most_freq:\n",
    "        result[lang] = len([word for word in tokenize(remove_tags(text.replace('\\n', '').lower())) if word in most_freq[lang]])\n",
    "    return sorted(result, key=lambda w: result[w], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Второй метод: частотные символьные n-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Преобразование строки в массив n-грамм заданной длины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice, tee\n",
    "\n",
    "def make_ngrams(text):\n",
    "    N = 3 # задаем длину n-граммы\n",
    "    ngrams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, N))))\n",
    "    ngrams = [''.join(x) for x in ngrams]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Список частотных ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frequent_ngram_list(langs, wiki_texts):\n",
    "    lang_freqs = {}\n",
    "    for lang in wiki_texts:\n",
    "        corpus = wiki_texts[lang]\n",
    "        lang_freqs[lang] = collections.defaultdict(lambda: 0)\n",
    "        for article in corpus:\n",
    "            for ngram in make_ngrams(remove_tags(article.replace('\\n', '').lower())):\n",
    "                lang_freqs[lang][ngram] += 1\n",
    "    return lang_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_freq_ngram(langs, wiki_texts):\n",
    "    # составляем частотный список\n",
    "    lang_freqs = frequent_ngram_list(langs, wiki_texts)\n",
    "    # фильтруем ngramы\n",
    "    lang_freqs_filtered = filter_tokens(lang_freqs)\n",
    "    # достаём самые частотные ngramы\n",
    "    most_freq = most_frequent(lang_freqs_filtered, langs, 300)\n",
    "    return most_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Определение языка n-граммным методом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_method(text, most_freq):\n",
    "    result = {}\n",
    "    for lang in most_freq:\n",
    "        result[lang] = len([ngram for ngram in make_ngrams(remove_tags(text.replace('\\n', '').lower())) if ngram in most_freq[lang]])\n",
    "    return sorted(result, key=lambda w: result[w], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Определяем язык двумя методами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_lang(filename, most_freq, most_freq_n):\n",
    "    text = open(filename,'r',encoding='utf-8').read()\n",
    "    print('Корпусный метод: ', corpus_method(text, most_freq))\n",
    "    print('n-граммный метод: ', ngram_method(text, most_freq_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_freq = main_freq(langs, wiki_texts)\n",
    "most_freq_n = main_freq_ngram(langs, wiki_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Корпусный метод:  fr\n",
      "n-граммный метод:  fr\n"
     ]
    }
   ],
   "source": [
    "predict_lang('/Users/irene/Downloads/HW2/fr.txt', most_freq, most_freq_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
